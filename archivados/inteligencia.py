import chromadb
from chromadb.utils import embedding_functions
from transformers import pipeline  # Usaremos transformers

# 1. Configuramos el cliente para usar la base persistente
client = chromadb.PersistentClient(path="chatbot/datos/db_biblia")

# 2. Definimos la funci칩n de embeddings
embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="all-MiniLM-L6-v2"
)

# 3. Cargamos la colecci칩n existente
collection = client.get_or_create_collection(name="versiculos", embedding_function=embedding_func)

# Crear el pipeline para generaci칩n de texto con el modelo GPT-2
chatbot = pipeline("text-generation", model="gpt2")

def responder_pregunta(pregunta: str, k=5):
    # Obtener los resultados de los vers칤culos m치s relevantes
    resultados = collection.query(
        query_texts=[pregunta],
        n_results=k
    )

    versiculos = resultados["documents"][0]
    contexto = "\n".join(versiculos)

    # Generar la respuesta usando GPT-2 (transformers)
    prompt = f"""
    Sos un experto en la Biblia. Respond칠 la siguiente pregunta de forma clara y directa,
    bas치ndote exclusivamente en los siguientes vers칤culos:

    {contexto}

    Pregunta: {pregunta}
    Respuesta:
    """

    # Generar respuesta
    respuesta = chatbot(prompt, max_length=100)[0]['generated_text']
    
    # Mostrar la respuesta
    print(f"\n游댍 Pregunta: {pregunta}")
    print("\n游닀 Respuesta:")
    print(respuesta)


if __name__ == "__main__":
    responder_pregunta("쮺u치ntos d칤as estuvo Jes칰s en el desierto?")
